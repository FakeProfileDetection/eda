{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb56f1f3",
   "metadata": {},
   "source": [
    "# Explore mapping new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b01733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/loripickering/Projects/eda/venv-3.12.5/bin/python\n",
      "Python 3.12.10\n",
      "/Users/loripickering/Projects/eda\n",
      "Loris-MBP.cable.rcn.com\n"
     ]
    }
   ],
   "source": [
    "! which python\n",
    "! python --version\n",
    "! pwd\n",
    "! hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53e7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "from pathlib3x import Path\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332fcac",
   "metadata": {},
   "source": [
    "# Explore files with complete entries\n",
    "\n",
    "These should have a \\_17.csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d21fb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files in ./uploads directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./uploads/t_af3ee429ac49c0b5d40c9bd7096e1984_17.csv',\n",
       " './uploads/t_5e484efa567c15fe10cb2484e226977f_17.csv',\n",
       " './uploads/t_a6274ed882811be8f50e798e0069460f_17.csv',\n",
       " './uploads/t_96,230,63,157,249,120,80,229,223,198,96,149,124,148,158,240_17.csv',\n",
       " './uploads/t_0d177a2c665478f45ab51fa8588e79c4_17.csv',\n",
       " './uploads/t_7d37027de313bd1dccc7e7533010e9e1_17.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use glob to get all files in the ./uploads directory if they end in \"_17.csv\"\n",
    "\n",
    "files = glob.glob(\"./uploads/*_17.csv\") \n",
    "print(f\"Found {len(files)} files in ./uploads directory.\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa89c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 6 user IDs from file names.\n",
      "['af3ee429ac49c0b5d40c9bd7096e1984', '5e484efa567c15fe10cb2484e226977f', 'a6274ed882811be8f50e798e0069460f', '96,230,63,157,249,120,80,229,223,198,96,149,124,148,158,240', '0d177a2c665478f45ab51fa8588e79c4', '7d37027de313bd1dccc7e7533010e9e1']\n"
     ]
    }
   ],
   "source": [
    "# Extract user ids from the file names\n",
    "user_ids = [Path(file).stem.split('_')[1] for file in files]\n",
    "print(f\"Extracted {len(user_ids)} user IDs from file names.\")\n",
    "print(user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ab769",
   "metadata": {},
   "source": [
    "# Functions for copying raw files to new user directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_file_mapping():\n",
    "    \"\"\"\n",
    "    Create a mapping from user_id to file paths.\n",
    "    \n",
    "    returns:\n",
    "        dict: A dictionary where keys are user_ids and values are lists of file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define mapping from file to user_id, platform_id, session_id, video_id:\n",
    "    platform_1 = [i for i in range(0, 17, 3)]\n",
    "    platform_2 = [i + 1 for i in platform_1]\n",
    "    platform_3 = [i + 2 for i in platform_1]\n",
    "\n",
    "    # Get session mapping\n",
    "    session_1 = [i for i in range(0,9)]\n",
    "    session_2 = [i  for i in range(9,18)]\n",
    "\n",
    "    # Get video indexes\n",
    "    video_1 = [0,1,2, 9,10,11]\n",
    "    video_2 = [i + 3 for i in video_1]\n",
    "    video_3 = [i + 3 for i in video_2]\n",
    "\n",
    "    # Get mapping from text (front of filename) to platform index\n",
    "    platform_idx_to_text = {idx: 'f' for idx in platform_1} \n",
    "    platform_idx_to_text.update({idx: 'i' for idx in platform_2})\n",
    "    platform_idx_to_text.update({idx: 't' for idx in platform_3})\n",
    "\n",
    "    # Get mapping of file index to platform indexes\n",
    "    file_index_to_platform = {idx: 1 for idx in platform_1}\n",
    "    file_index_to_platform.update({idx: 2 for idx in platform_2})\n",
    "    file_index_to_platform.update({idx: 3 for idx in platform_3})\n",
    "\n",
    "    # Get mapping of file index to session and video indexes\n",
    "    file_index_to_session = {idx: 1 for idx in session_1}\n",
    "    file_index_to_session.update({idx: 2 for idx in session_2})\n",
    "\n",
    "    file_index_to_video = {idx: 1 for idx in video_1}\n",
    "    file_index_to_video.update({idx: 2 for idx in video_2})\n",
    "    file_index_to_video.update({idx: 3 for idx in video_3})\n",
    "\n",
    "    # Create a map from each file to user_id, platform_id, session_id, video_id\n",
    "    # where each file index is mapped to its user_id, platform_id, session_id, video_id\n",
    "    files_mapping = {}\n",
    "    for u in user_ids:\n",
    "        for file_index in range(0,18):\n",
    "            file = f\"./uploads/{platform_idx_to_text[file_index]}_{u}_{file_index}.csv\"\n",
    "            # print(f\"Processing file: {file}, platform: {file_index_to_platform[file_index]}, session: {file_index_to_session[file_index]}, video: {file_index_to_video[file_index]}\")\n",
    "            files_mapping[file] = {\n",
    "                \"user_id\": u,\n",
    "                \"platform_id\": file_index_to_platform[file_index],\n",
    "                \"session_id\": file_index_to_session[file_index],\n",
    "                \"video_id\": file_index_to_video[file_index]\n",
    "            }\n",
    "            \n",
    "    return files_mapping\n",
    "\n",
    "def move_to_broken_data_dir(new_data_dir, user_id):\n",
    "    \"\"\"\n",
    "    Moves the user directory to the broken_data directory.if the directory already exists, de\n",
    "    \n",
    "    :param new_data_dir: Path to the directory where the user directories are stored.\n",
    "    :param user_id: The user_id of the directory to move.\n",
    "    \n",
    "    returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    user_dir = new_data_dir / user_id\n",
    "    broken_data_dir = new_data_dir / \"broken_data\"\n",
    "    broken_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if user_dir.exists():\n",
    "        # print(f\"Moving {user_dir} to {broken_data_dir}\")\n",
    "        for file in user_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                # Move each file to the broken_data directory\n",
    "                os.system(f\"cp {file} {broken_data_dir / file.name}\")\n",
    "                os.system(f\"rm {file}\")\n",
    "        # Remove the user directory after moving files\n",
    "        user_dir.rmdir()\n",
    "        # print(f\"Moved {user_dir} to {broken_data_dir}\")\n",
    "    else:\n",
    "        # print(f\"User directory {user_dir} does not exist, skipping.\")\n",
    "        pass\n",
    "        \n",
    "        \n",
    "def copy_files_to_user_dirs(old_data_dir, new_data_dir, files_mapping):\n",
    "    \"\"\"\n",
    "    Copies files from old_data_dir to new_data_dir, renaming them according to the mapping.\n",
    "    \n",
    "    :param old_data_dir: Path to the directory containing the original files.\n",
    "    :param new_data_dir: Path to the directory where the renamed files will be saved.\n",
    "    :param files_mapping: Dictionary mapping file paths to user_id, platform_id, session_id, video_id.\n",
    "    \n",
    "    saves files not found and files found to csv files in the new_data_dir.\n",
    "    \n",
    "    returns:\n",
    "    - csv file describing files that are missing or could not be copied\n",
    "    - csv file describing files that were successfully copied\n",
    "    \"\"\"\n",
    "    \n",
    "    files_not_found = []\n",
    "    files_copied = []\n",
    "    for file, mapping in files_mapping.items():\n",
    "        # create user_id directory if it doesn't exist\n",
    "        user_dir = new_data_dir / mapping['user_id']\n",
    "        user_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        new_file_name = f\"{mapping['platform_id']}_{mapping['video_id']}_{mapping['session_id']}_{mapping['user_id']}.csv\"\n",
    "        new_file_path = user_dir / new_file_name\n",
    "        old_path = Path(file) / file\n",
    "        \n",
    "        # Copy the file to the new location\n",
    "        try:\n",
    "            Path(file).copy(new_file_path)\n",
    "            # print(f\"Copied {file} to {new_file_path}\")\n",
    "            files_copied.append({\n",
    "                \"old_file\": file,\n",
    "                \"new_file\": str(new_file_path),\n",
    "                \"user_id\": mapping['user_id'],\n",
    "                \"platform_id\": mapping['platform_id'],\n",
    "                \"session_id\": mapping['session_id'],\n",
    "                \"video_id\": mapping['video_id']\n",
    "            })\n",
    "        except FileNotFoundError:\n",
    "            # print(f\"File not found: {file}\")\n",
    "            files_not_found.append({\n",
    "                \"file\": file,\n",
    "                \"user_id\": mapping['user_id'],\n",
    "                \"platform_id\": mapping['platform_id'],\n",
    "                \"session_id\": mapping['session_id'],\n",
    "                \"video_id\": mapping['video_id'],\n",
    "                \"error\": \"File not found\"\n",
    "            })\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # print(f\"Error copying {file} to {new_file_path}: {e}\")\n",
    "            files_not_found.append({\n",
    "                \"file\": file,\n",
    "                \"user_id\": mapping['user_id'],\n",
    "                \"platform_id\": mapping['platform_id'],\n",
    "                \"session_id\": mapping['session_id'],\n",
    "                \"video_id\": mapping['video_id'],\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "    # Save the files_not_found and files_copied to CSV files\n",
    "    if files_not_found:\n",
    "        not_found_df = pl.DataFrame(files_not_found)\n",
    "        not_found_df.write_csv(new_data_dir / \"files_not_found.csv\")\n",
    "        # print(f\"Saved {len(files_not_found)} files not found to {new_data_dir / 'files_not_found.csv'}\")\n",
    "    else:\n",
    "        not_found_df = None\n",
    "    if files_copied:\n",
    "        copied_df = pl.DataFrame(files_copied)\n",
    "        copied_df.write_csv(new_data_dir / \"files_copied.csv\")\n",
    "        # print(f\"Saved {len(files_copied)} files copied to {new_data_dir / 'files_copied.csv'}\")\n",
    "    else:\n",
    "        copied_df = None\n",
    "        \n",
    "    # Test each file in copied_df to see if it can be read into a polars DataFrame\n",
    "    if copied_df is not None:\n",
    "        bad_files = []\n",
    "        bad_user_ids = set(not_found_df['user_id'].to_list()) if not_found_df is not None else set()\n",
    "        for row in copied_df.iter_rows(named=True):\n",
    "            user_id = row['user_id']\n",
    "            file = row['new_file']\n",
    "            # load into polars dataframe\n",
    "            if user_id not in bad_user_ids:\n",
    "                continue\n",
    "            try:\n",
    "                df = pl.read_csv(file, has_header=False, infer_schema_length=5000)\n",
    "                # print(f\"Loaded {file} with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "            except Exception as e:\n",
    "                # print(f\"Error loading {file}: {e}\")\n",
    "                bad_files.append(file)\n",
    "                continue\n",
    "            \n",
    "        # Add bad_files to not_found_df\n",
    "        if bad_files:\n",
    "            for file in bad_files:\n",
    "                user_id = Path(file).parent.name\n",
    "                platform_id, video_id, session_id, _ = Path(file).stem.split('_')\n",
    "                not_found_df = not_found_df.append({\n",
    "                    \"file\": file,\n",
    "                    \"user_id\": user_id,\n",
    "                    \"platform_id\": platform_id,\n",
    "                    \"session_id\": session_id,\n",
    "                    \"video_id\": video_id,\n",
    "                    \"error\": \"File could not be read into a DataFrame\"\n",
    "                }, ignore_index=True)\n",
    "        \n",
    "        \n",
    "    # Copy data from user ids in not_found_df to \"broken_data\" directory\n",
    "    if not_found_df is not None:\n",
    "        bad_user_ids = not_found_df['user_id'].unique().to_list() if not_found_df is not None else []\n",
    "        for user_id in bad_user_ids:\n",
    "            move_to_broken_data_dir(new_data_dir, user_id)\n",
    "        \n",
    "        # Remove bad user ids data from copied_df\n",
    "        if copied_df is not None:\n",
    "            copied_df = copied_df.filter(~pl.col(\"user_id\").is_in(bad_user_ids))\n",
    "            if not copied_df.is_empty():\n",
    "                copied_df.write_csv(new_data_dir / \"files_copied.csv\")\n",
    "                # print(f\"Updated files_copied.csv with remaining files: {new_data_dir / 'files_copied.csv'}\")\n",
    "            else:\n",
    "                pass\n",
    "                # print(\"No files left in files_copied after removing bad user ids.\")\n",
    "    else:\n",
    "        print(\"No files not found, skipping moving user directories to broken_data.\")\n",
    "        \n",
    "    if files_not_found:\n",
    "        not_found_df.write_csv(new_data_dir / \"files_not_found.csv\")\n",
    "        # print(f\"Saved {len(files_not_found)} files not found to {new_data_dir / 'files_not_found.csv'}\")\n",
    "\n",
    "    if files_copied:\n",
    "        copied_df.write_csv(new_data_dir / \"files_copied.csv\")\n",
    "        # print(f\"Saved {len(files_copied)} files copied to {new_data_dir / 'files_copied.csv'}\")\n",
    "            \n",
    "    \n",
    "    return not_found_df, copied_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d0ce5",
   "metadata": {},
   "source": [
    "# Test functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f997a5e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 66] Directory not empty: 'new_data/5e484efa567c15fe10cb2484e226977f'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m new_data_dir = Path(\u001b[33m\"\u001b[39m\u001b[33m./new_data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m files_mapping = get_user_file_mapping()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m not_found_df, copied_df  = \u001b[43mcopy_files_to_user_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Get get user ids from copied_df\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copied_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mcopy_files_to_user_dirs\u001b[39m\u001b[34m(old_data_dir, new_data_dir, files_mapping)\u001b[39m\n\u001b[32m    195\u001b[39m bad_user_ids = not_found_df[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m].unique().to_list() \u001b[38;5;28;01mif\u001b[39;00m not_found_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m bad_user_ids:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[43mmove_to_broken_data_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Remove bad user ids data from copied_df\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copied_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mmove_to_broken_data_dir\u001b[39m\u001b[34m(new_data_dir, user_id)\u001b[39m\n\u001b[32m     76\u001b[39m             os.system(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbroken_data_dir\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39mfile.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Remove the user directory after moving files\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[43muser_dir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# print(f\"Moved {user_dir} to {broken_data_dir}\")\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# print(f\"User directory {user_dir} does not exist, skipping.\")\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/eda/venv-3.12.5/lib/python3.12/site-packages/pathlib3x/pathlib3x.py:1254\u001b[39m, in \u001b[36mPath.rmdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrmdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1251\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1252\u001b[39m \u001b[33;03m    Remove this directory.  The directory must be empty.\u001b[39;00m\n\u001b[32m   1253\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 66] Directory not empty: 'new_data/5e484efa567c15fe10cb2484e226977f'"
     ]
    }
   ],
   "source": [
    "raw_data_dir = Path(\"./uploads\")\n",
    "new_data_dir = Path(\"./new_data\")\n",
    "\n",
    "files_mapping = get_user_file_mapping()\n",
    "not_found_df, copied_df  = copy_files_to_user_dirs(raw_data_dir, new_data_dir, files_mapping)\n",
    "\n",
    "# Get get user ids from copied_df\n",
    "if copied_df is not None:\n",
    "    user_ids_from_copied = copied_df['user_id'].unique().to_list()\n",
    "    print(f\"User IDs from copied files: {user_ids_from_copied}\")\n",
    "else:\n",
    "    user_ids_from_copied = []\n",
    "    \n",
    "print(f\"Total good user IDs: {len(user_ids_from_copied)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce946d5",
   "metadata": {},
   "source": [
    "## Verify files copied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2b8019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! ls -l ./new_data/*/*.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381874a1",
   "metadata": {},
   "source": [
    "# Verfify all file can be loaded into a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e399d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_files = []\n",
    "bad_user_ids = set(not_found_df['user_id'].to_list())\n",
    "\n",
    "for row in copied_df.iter_rows(named=True):\n",
    "    user_id = row['user_id']\n",
    "    file = row['new_file']\n",
    "    # load into polars dataframe\n",
    "    if user_id not in bad_user_ids:\n",
    "        continue\n",
    "    try:\n",
    "        df = pl.read_csv(file, has_header=False, infer_schema_length=5000)\n",
    "        # print(f\"Loaded {file} with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "    except Exception as e:\n",
    "        # print(f\"Error loading {file}: {e}\")\n",
    "        bad_files.append(file)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f122e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files that could not be loaded.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(bad_files)} files that could not be loaded.\")\n",
    "print(bad_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c648226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd6f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdde973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
